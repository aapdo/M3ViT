# Setup
setup: pretrain

# Database
train_db_name: ImageNet1K
val_db_name: ImageNet1K
trBatch: 64
valBatch: 96
nworkers: 10
input_size: 224
nb_classes: 1000

# Optimizer and scheduler (V-MoE-inspired, adapted to this repo's cosine scheduler)
epochs: 300
optimizer: adamw
optimizer_kwargs:
  lr: 0.001
  momentum: 0.9
  weight_decay: 0.1
  opt_betas: [0.9, 0.999]
  opt_eps: 1.0e-8
scheduler: cosine
scheduler_kwargs:
  warmup_epochs: 10
  min_lr: 1.0e-5
  unscale_lr: true
clip_grad: 1.0

# Model
model: moe_vit_small
backbone: VisionTransformer_moe
backbone_kwargs:
  model_name: vit_small_patch16_224
  img_size: 224
  patch_size: 16
  in_chans: 3
  embed_dim: 384
  depth: 12
  num_heads: 6
  num_classes: 1000

  drop_rate: 0.0
  pos_embed_interp: false
  align_corners: false

  mlp_ratio: 4.0
  qkv_bias: true
  attn_drop_rate: 0.0
  drop_path_rate: 0.0

  random_init: true
  deit_init_mode: scratch
  distilled: false
  moe_mlp_ratio: 2.0
  moe_experts: 8
  moe_top_k: 2
  gate_dim: -1
  gate_task_specific_dim: -1
  multi_gate: false
  moe_gate_type: noisy_vmoe
  vmoe_noisy_std: 1.0

# Router aux loss proxy:
# This codebase uses cv_loss = cv(importance) + cv(load), scaled by moe_cv_weight.
# V-MoE uses 0.005 for each term, so 0.005 is the closest equivalent here.
moe_cv_weight: 0.005

# Augmentation (V-MoE light1 style approximation in timm)
aug_kwargs:
  color_jitter: 0.0
  aa: rand-m10-mstd0.5-inc1
  train_interpolation: bicubic
  reprob: 0.0
  remode: pixel
  recount: 1
  repeated_aug: false

  mixup: 0.2
  cutmix: 0.0
  mixup_prob: 1.0
  mixup_switch_prob: 0.0
  mixup_mode: batch
  smoothing: 0.0

# Distillation
distillation_kwargs:
  distillation_type: none
  teacher_model: regnety_160
  teacher_path: "https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth"
  distillation_alpha: 0.5
  distillation_tau: 1.0

# Runtime
seed: 0
amp: true
pin_mem: true
dist_eval: false
model_ema: false
print_freq: 50
save_freq: 10
