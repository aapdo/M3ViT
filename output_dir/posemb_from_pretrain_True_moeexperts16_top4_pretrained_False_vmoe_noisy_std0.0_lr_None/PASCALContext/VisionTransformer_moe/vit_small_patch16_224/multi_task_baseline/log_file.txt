Set CuDNN benchmark
Set random seed to 0, deterministic: False
{'setup': 'multi_task', 'train_db_name': 'PASCALContext', 'val_db_name': 'PASCALContext', 'trBatch': 8, 'valBatch': 8, 'nworkers': 4, 'epochs': 5, 'optimizer': 'sgd', 'optimizer_kwargs': {'lr': 0.002, 'momentum': 0.9, 'weight_decay': 0.0001}, 'scheduler': 'poly', 'model': 'baseline', 'model_kwargs': {'tam': True, 'tam_level0': False, 'tam_level1': False, 'tam_level2': False}, 'backbone': 'VisionTransformer_moe', 'backbone_kwargs': {'dilated': False, 'model_name': 'vit_small_patch16_224', 'img_size': [512, 512], 'patch_size': 16, 'in_chans': 3, 'embed_dim': 384, 'depth': 12, 'num_heads': 12, 'num_classes': 40, 'drop_rate': 0.0, 'pos_embed_interp': True, 'align_corners': False, 'mlp_ratio': 4.0, 'qkv_bias': 'True,', 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'random_init': False, 'distilled': False, 'moe_mlp_ratio': 1, 'moe_top_k': 4, 'gate_dim': 389, 'gate_return_decoupled_activation': False}, 'head': 'VisionTransformerUpHead', 'head_kwargs': {'in_channels': 1024, 'channels': 512, 'embed_dim': 384, 'img_size': [512, 512], 'align_corners': False, 'num_conv': 4, 'upsampling_method': 'bilinear', 'num_upsampe_layer': 4, 'num_classes': 40, 'in_index': 11, 'patch_size': 16, 'conv3x3_conv1x1': True}, 'task_dictionary': {'include_semseg': True, 'include_human_parts': True, 'include_sal': True, 'include_edge': True, 'include_normals': True, 'edge_w': 0.95}, 'loss_kwargs': {'loss_scheme': 'baseline', 'loss_weights': {'semseg': 1.0, 'human_parts': 2.0, 'sal': 5.0, 'edge': 50.0, 'normals': 10.0}}, 'eval_final_10_epochs_only': True, 'TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}, 'SINGLE_TASK_TEST_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_test_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_test_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_test_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_test_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_test_edge.json'}, 'SINGLE_TASK_VAL_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_val_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_val_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_val_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_val_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_val_edge.json'}}, 'normloss': 1, 'edge_w': 0.95, 'eval_edge': False, 'ALL_TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}}, 'TRAIN': {'SCALE': (512, 512)}, 'TEST': {'SCALE': (512, 512)}, 'overfit': False, 'multi_level': False, 'root_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None', 'output_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline', 'save_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/results', 'checkpoint': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/checkpoint.pth.tar', 'best_model': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/best_model.pth.tar', 'multi_gate': True, 'local_rank': 0, 'gpus': 2}
Distributed training: True
torch.backends.cudnn.benchmark: True
Namespace(config_env='configs/env.yml', config_exp='configs/pascal/vit_moe/pup_moe_vit_small_multi_task_baseline_onehot.yml', gpus=1, launcher='pytorch', local_rank=0, seed=0, deterministic=False, moe_data_distributed=False, moe_experts=16, moe_mlp_ratio=1, moe_top_k=4, trBatch=None, valBatch=None, moe_gate_arch='', moe_gate_type='noisy_vmoe', vmoe_noisy_std=0.0, backbone_random_init=False, pretrained='', moe_noisy_gate_loss_weight=0.01, pos_emb_from_pretrained='True', lr=None, one_by_one=False, task_one_hot=False, multi_gate=True, eval=False, flops=False, ckp=None, save_dir='/home/jy/m3vit/output_dir', gate_task_specific_dim=-1, regu_experts_fromtask=False, num_experts_pertask=-1, gate_input_ahead=False, regu_sem=False, semregu_loss_weight=0.01, sem_force=False, warmup_epochs=5, epochs=None, regu_subimage=False, subimageregu_weight=0.01, multi_level=None, opt=None, weight_decay=0.0001, expert_prune=False, tam_level0=None, tam_level1=None, tam_level2=None, resume='', time=False, forward_hook=False, num_tasks=5, distributed=True, world_size=2)
Retrieve model
/usr/local/lib/python3.10/dist-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
================================================================================
Creating: models.vision_transformer_moe.VisionTransformerMoE
  Arguments:
    model_name: vit_small_patch16_224
    img_size: [512, 512]
    patch_size: 16
    in_chans: 3
    embed_dim: 384
    depth: 12
    num_heads: 12
    num_classes: 40
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    representation_size: None
    distilled: False
    drop_rate: 0.0
    attn_drop_rate: 0.0
    drop_path_rate: 0.0
    hybrid_backbone: None
    norm_cfg: {'type': 'SyncBN', 'requires_grad': True}
    pos_embed_interp: True
    random_init: False
    align_corners: False
    act_layer: None
    weight_init: 
    moe_mlp_ratio: 1
    moe_experts: 8
    moe_top_k: 4
    world_size: 2
    gate_dim: 389
    gate_return_decoupled_activation: False
    moe_gate_type: noisy_vmoe
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    gate_input_ahead: False
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
set expert prune as  False
================================================================================
Creating: models.vision_transformer_moe.PatchEmbed
  Arguments:
    img_size: [512, 512]
    patch_size: 16
    in_chans: 3
    embed_dim: 384
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Mlp
  Arguments:
    in_features: 384
    hidden_features: 1536
    act_layer: <class 'torch.nn.modules.activation.GELU'>
    drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
    moe: True
    moe_mlp_ratio: 1
    moe_experts: 8
    moe_top_k: 4
    moe_gate_dim: 389
    world_size: 2
    gate_return_decoupled_activation: False
    moe_gate_type: noisy_vmoe
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    gate_input_ahead: False
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.custom_moe_layer.FMoETransformerMLP
  Arguments:
    num_expert: 8
    d_model: 384
    d_gate: 389
    d_hidden: 384
    world_size: 2
    top_k: 4
    activation: Sequential module
    gate: <class 'models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE'>
    gate_return_decoupled_activation: False
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
================================================================================
================================================================================
Creating: models.custom_moe_layer._Expert
  Arguments:
    num_expert: 8
    d_model: 384
    d_hidden: 384
    activation: Sequential module
    rank: 0
================================================================================
multi_gate True
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Mlp
  Arguments:
    in_features: 384
    hidden_features: 1536
    act_layer: <class 'torch.nn.modules.activation.GELU'>
    drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
    moe: True
    moe_mlp_ratio: 1
    moe_experts: 8
    moe_top_k: 4
    moe_gate_dim: 389
    world_size: 2
    gate_return_decoupled_activation: False
    moe_gate_type: noisy_vmoe
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    gate_input_ahead: False
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.custom_moe_layer.FMoETransformerMLP
  Arguments:
    num_expert: 8
    d_model: 384
    d_gate: 389
    d_hidden: 384
    world_size: 2
    top_k: 4
    activation: Sequential module
    gate: <class 'models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE'>
    gate_return_decoupled_activation: False
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
================================================================================
================================================================================
Creating: models.custom_moe_layer._Expert
  Arguments:
    num_expert: 8
    d_model: 384
    d_hidden: 384
    activation: Sequential module
    rank: 0
================================================================================
multi_gate True
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Mlp
  Arguments:
    in_features: 384
    hidden_features: 1536
    act_layer: <class 'torch.nn.modules.activation.GELU'>
    drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
    moe: True
    moe_mlp_ratio: 1
    moe_experts: 8
    moe_top_k: 4
    moe_gate_dim: 389
    world_size: 2
    gate_return_decoupled_activation: False
    moe_gate_type: noisy_vmoe
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    gate_input_ahead: False
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.custom_moe_layer.FMoETransformerMLP
  Arguments:
    num_expert: 8
    d_model: 384
    d_gate: 389
    d_hidden: 384
    world_size: 2
    top_k: 4
    activation: Sequential module
    gate: <class 'models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE'>
    gate_return_decoupled_activation: False
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
================================================================================
================================================================================
Creating: models.custom_moe_layer._Expert
  Arguments:
    num_expert: 8
    d_model: 384
    d_hidden: 384
    activation: Sequential module
    rank: 0
================================================================================
multi_gate True
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Mlp
  Arguments:
    in_features: 384
    hidden_features: 1536
    act_layer: <class 'torch.nn.modules.activation.GELU'>
    drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
    moe: True
    moe_mlp_ratio: 1
    moe_experts: 8
    moe_top_k: 4
    moe_gate_dim: 389
    world_size: 2
    gate_return_decoupled_activation: False
    moe_gate_type: noisy_vmoe
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    gate_input_ahead: False
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.custom_moe_layer.FMoETransformerMLP
  Arguments:
    num_expert: 8
    d_model: 384
    d_gate: 389
    d_hidden: 384
    world_size: 2
    top_k: 4
    activation: Sequential module
    gate: <class 'models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE'>
    gate_return_decoupled_activation: False
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
================================================================================
================================================================================
Creating: models.custom_moe_layer._Expert
  Arguments:
    num_expert: 8
    d_model: 384
    d_hidden: 384
    activation: Sequential module
    rank: 0
================================================================================
multi_gate True
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Mlp
  Arguments:
    in_features: 384
    hidden_features: 1536
    act_layer: <class 'torch.nn.modules.activation.GELU'>
    drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
    moe: True
    moe_mlp_ratio: 1
    moe_experts: 8
    moe_top_k: 4
    moe_gate_dim: 389
    world_size: 2
    gate_return_decoupled_activation: False
    moe_gate_type: noisy_vmoe
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    gate_input_ahead: False
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.custom_moe_layer.FMoETransformerMLP
  Arguments:
    num_expert: 8
    d_model: 384
    d_gate: 389
    d_hidden: 384
    world_size: 2
    top_k: 4
    activation: Sequential module
    gate: <class 'models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE'>
    gate_return_decoupled_activation: False
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
================================================================================
================================================================================
Creating: models.custom_moe_layer._Expert
  Arguments:
    num_expert: 8
    d_model: 384
    d_hidden: 384
    activation: Sequential module
    rank: 0
================================================================================
multi_gate True
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Mlp
  Arguments:
    in_features: 384
    hidden_features: 1536
    act_layer: <class 'torch.nn.modules.activation.GELU'>
    drop: 0.0
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Block
  Arguments:
    dim: 384
    num_heads: 12
    mlp_ratio: 4.0
    qkv_bias: True,
    qk_scale: None
    drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    norm_layer: functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06)
    moe: True
    moe_mlp_ratio: 1
    moe_experts: 8
    moe_top_k: 4
    moe_gate_dim: 389
    world_size: 2
    gate_return_decoupled_activation: False
    moe_gate_type: noisy_vmoe
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    gate_input_ahead: False
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.vision_transformer_moe.Attention
  Arguments:
    dim: 384
    num_heads: 12
    qkv_bias: True,
    qk_scale: None
    attn_drop: 0.0
    proj_drop: 0.0
================================================================================
================================================================================
Creating: models.custom_moe_layer.FMoETransformerMLP
  Arguments:
    num_expert: 8
    d_model: 384
    d_gate: 389
    d_hidden: 384
    world_size: 2
    top_k: 4
    activation: Sequential module
    gate: <class 'models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE'>
    gate_return_decoupled_activation: False
    vmoe_noisy_std: 0.0
    gate_task_specific_dim: -1
    multi_gate: True
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
    expert_prune: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
================================================================================
================================================================================
Creating: models.custom_moe_layer._Expert
  Arguments:
    num_expert: 8
    d_model: 384
    d_hidden: 384
    activation: Sequential module
    rank: 0
================================================================================
multi_gate True
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
================================================================================
Creating: models.gate_funs.noisy_gate_vmoe.NoisyGate_VMoE
  Arguments:
    d_model: 384
    num_expert: 8
    world_size: 2
    top_k: 4
    return_decoupled_activation: False
    noise_std: 0.0
    regu_experts_fromtask: False
    num_experts_pertask: -1
    num_tasks: 5
    regu_sem: False
    sem_force: False
    regu_subimage: False
================================================================================
=========pos emb is loaded from ================ https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth
load pre-trained weight from imagenet21k
loaded pos_embed shape torch.Size([1, 197, 384])
torch.Size([1, 384, 14, 14])
after interpolation torch.Size([1, 384, 32, 32])
cls_token_weight torch.Size([1, 1, 384])
============load model weights from============ https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth _IncompatibleKeys(missing_keys=['blocks.1.mlp.gate.0.w_gate', 'blocks.1.mlp.gate.1.w_gate', 'blocks.1.mlp.gate.2.w_gate', 'blocks.1.mlp.gate.3.w_gate', 'blocks.1.mlp.gate.4.w_gate', 'blocks.1.mlp.experts.htoh4.weight', 'blocks.1.mlp.experts.htoh4.bias', 'blocks.1.mlp.experts.h4toh.weight', 'blocks.1.mlp.experts.h4toh.bias', 'blocks.3.mlp.gate.0.w_gate', 'blocks.3.mlp.gate.1.w_gate', 'blocks.3.mlp.gate.2.w_gate', 'blocks.3.mlp.gate.3.w_gate', 'blocks.3.mlp.gate.4.w_gate', 'blocks.3.mlp.experts.htoh4.weight', 'blocks.3.mlp.experts.htoh4.bias', 'blocks.3.mlp.experts.h4toh.weight', 'blocks.3.mlp.experts.h4toh.bias', 'blocks.5.mlp.gate.0.w_gate', 'blocks.5.mlp.gate.1.w_gate', 'blocks.5.mlp.gate.2.w_gate', 'blocks.5.mlp.gate.3.w_gate', 'blocks.5.mlp.gate.4.w_gate', 'blocks.5.mlp.experts.htoh4.weight', 'blocks.5.mlp.experts.htoh4.bias', 'blocks.5.mlp.experts.h4toh.weight', 'blocks.5.mlp.experts.h4toh.bias', 'blocks.7.mlp.gate.0.w_gate', 'blocks.7.mlp.gate.1.w_gate', 'blocks.7.mlp.gate.2.w_gate', 'blocks.7.mlp.gate.3.w_gate', 'blocks.7.mlp.gate.4.w_gate', 'blocks.7.mlp.experts.htoh4.weight', 'blocks.7.mlp.experts.htoh4.bias', 'blocks.7.mlp.experts.h4toh.weight', 'blocks.7.mlp.experts.h4toh.bias', 'blocks.9.mlp.gate.0.w_gate', 'blocks.9.mlp.gate.1.w_gate', 'blocks.9.mlp.gate.2.w_gate', 'blocks.9.mlp.gate.3.w_gate', 'blocks.9.mlp.gate.4.w_gate', 'blocks.9.mlp.experts.htoh4.weight', 'blocks.9.mlp.experts.htoh4.bias', 'blocks.9.mlp.experts.h4toh.weight', 'blocks.9.mlp.experts.h4toh.bias', 'blocks.11.mlp.gate.0.w_gate', 'blocks.11.mlp.gate.1.w_gate', 'blocks.11.mlp.gate.2.w_gate', 'blocks.11.mlp.gate.3.w_gate', 'blocks.11.mlp.gate.4.w_gate', 'blocks.11.mlp.experts.htoh4.weight', 'blocks.11.mlp.experts.htoh4.bias', 'blocks.11.mlp.experts.h4toh.weight', 'blocks.11.mlp.experts.h4toh.bias'], unexpected_keys=['norm.weight', 'norm.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias'])
backbone VisionTransformerMoE(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): FMoETransformerMLP(
        (gate): ModuleList(
          (0-4): 5 x NoisyGate_VMoE(
            (softmax): Softmax(dim=1)
          )
        )
        (experts): _Expert(
          (htoh4): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (h4toh): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (activation): Sequential(
            (0): GELU(approximate='none')
            (1): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (mlp_drop): Dropout(p=0.0, inplace=False)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): FMoETransformerMLP(
        (gate): ModuleList(
          (0-4): 5 x NoisyGate_VMoE(
            (softmax): Softmax(dim=1)
          )
        )
        (experts): _Expert(
          (htoh4): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (h4toh): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (activation): Sequential(
            (0): GELU(approximate='none')
            (1): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (mlp_drop): Dropout(p=0.0, inplace=False)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): FMoETransformerMLP(
        (gate): ModuleList(
          (0-4): 5 x NoisyGate_VMoE(
            (softmax): Softmax(dim=1)
          )
        )
        (experts): _Expert(
          (htoh4): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (h4toh): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (activation): Sequential(
            (0): GELU(approximate='none')
            (1): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (mlp_drop): Dropout(p=0.0, inplace=False)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): FMoETransformerMLP(
        (gate): ModuleList(
          (0-4): 5 x NoisyGate_VMoE(
            (softmax): Softmax(dim=1)
          )
        )
        (experts): _Expert(
          (htoh4): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (h4toh): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (activation): Sequential(
            (0): GELU(approximate='none')
            (1): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (mlp_drop): Dropout(p=0.0, inplace=False)
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): FMoETransformerMLP(
        (gate): ModuleList(
          (0-4): 5 x NoisyGate_VMoE(
            (softmax): Softmax(dim=1)
          )
        )
        (experts): _Expert(
          (htoh4): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (h4toh): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (activation): Sequential(
            (0): GELU(approximate='none')
            (1): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (mlp_drop): Dropout(p=0.0, inplace=False)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): FMoETransformerMLP(
        (gate): ModuleList(
          (0-4): 5 x NoisyGate_VMoE(
            (softmax): Softmax(dim=1)
          )
        )
        (experts): _Expert(
          (htoh4): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (h4toh): FMoELinear(num_expert=8, in_features=384,         out_features=384, bias=True, rank=0)
          (activation): Sequential(
            (0): GELU(approximate='none')
            (1): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (mlp_drop): Dropout(p=0.0, inplace=False)
    )
  )
  (pre_logits): Identity()
)
================================================================================
Creating: models.vit_up_head.VisionTransformerUpHead
  Arguments:
    img_size: [512, 512]
    patch_size: 16
    embed_dim: 384
    norm_cfg: {'type': 'SyncBN', 'requires_grad': True}
    num_conv: 4
    upsampling_method: bilinear
    num_upsampe_layer: 4
    conv3x3_conv1x1: True
    p: {'setup': 'multi_task', 'train_db_name': 'PASCALContext', 'val_db_name': 'PASCALContext', 'trBatch': 8, 'valBatch': 8, 'nworkers': 4, 'epochs': 5, 'optimizer': 'sgd', 'optimizer_kwargs': {'lr': 0.002, 'momentum': 0.9, 'weight_decay': 0.0001}, 'scheduler': 'poly', 'model': 'baseline', 'model_kwargs': {'tam': True, 'tam_level0': False, 'tam_level1': False, 'tam_level2': False}, 'backbone': 'VisionTransformer_moe', 'backbone_kwargs': {'dilated': False, 'model_name': 'vit_small_patch16_224', 'img_size': [512, 512], 'patch_size': 16, 'in_chans': 3, 'embed_dim': 384, 'depth': 12, 'num_heads': 12, 'num_classes': 40, 'drop_rate': 0.0, 'pos_embed_interp': True, 'align_corners': False, 'mlp_ratio': 4.0, 'qkv_bias': 'True,', 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'random_init': False, 'distilled': False, 'moe_mlp_ratio': 1, 'moe_top_k': 4, 'gate_dim': 389, 'gate_return_decoupled_activation': False}, 'head': 'VisionTransformerUpHead', 'head_kwargs': {'in_channels': 1024, 'channels': 512, 'embed_dim': 384, 'img_size': [512, 512], 'align_corners': False, 'num_conv': 4, 'upsampling_method': 'bilinear', 'num_upsampe_layer': 4, 'num_classes': 40, 'in_index': 11, 'patch_size': 16, 'conv3x3_conv1x1': True}, 'task_dictionary': {'include_semseg': True, 'include_human_parts': True, 'include_sal': True, 'include_edge': True, 'include_normals': True, 'edge_w': 0.95}, 'loss_kwargs': {'loss_scheme': 'baseline', 'loss_weights': {'semseg': 1.0, 'human_parts': 2.0, 'sal': 5.0, 'edge': 50.0, 'normals': 10.0}}, 'eval_final_10_epochs_only': True, 'TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}, 'SINGLE_TASK_TEST_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_test_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_test_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_test_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_test_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_test_edge.json'}, 'SINGLE_TASK_VAL_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_val_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_val_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_val_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_val_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_val_edge.json'}}, 'normloss': 1, 'edge_w': 0.95, 'eval_edge': False, 'ALL_TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}}, 'TRAIN': {'SCALE': (512, 512)}, 'TEST': {'SCALE': (512, 512)}, 'overfit': False, 'multi_level': False, 'root_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None', 'output_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline', 'save_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/results', 'checkpoint': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/checkpoint.pth.tar', 'best_model': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/best_model.pth.tar', 'multi_gate': True, 'local_rank': 0, 'gpus': 2}
    in_channels: 1024
    channels: 512
    in_index: 11
    num_classes: 21
    align_corners: False
================================================================================
self.h 32 self.w 32
will consider multi level output in head False
will consider tam in heads True
head[semseg] VisionTransformerUpHead(
  input_transform=None, ignore_index=255, align_corners=False
  (dropout): Dropout2d(p=0.1, inplace=False)
  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (conv_0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))
  (syncbn_fc_0): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
================================================================================
Creating: models.vit_up_head.VisionTransformerUpHead
  Arguments:
    img_size: [512, 512]
    patch_size: 16
    embed_dim: 384
    norm_cfg: {'type': 'SyncBN', 'requires_grad': True}
    num_conv: 4
    upsampling_method: bilinear
    num_upsampe_layer: 4
    conv3x3_conv1x1: True
    p: {'setup': 'multi_task', 'train_db_name': 'PASCALContext', 'val_db_name': 'PASCALContext', 'trBatch': 8, 'valBatch': 8, 'nworkers': 4, 'epochs': 5, 'optimizer': 'sgd', 'optimizer_kwargs': {'lr': 0.002, 'momentum': 0.9, 'weight_decay': 0.0001}, 'scheduler': 'poly', 'model': 'baseline', 'model_kwargs': {'tam': True, 'tam_level0': False, 'tam_level1': False, 'tam_level2': False}, 'backbone': 'VisionTransformer_moe', 'backbone_kwargs': {'dilated': False, 'model_name': 'vit_small_patch16_224', 'img_size': [512, 512], 'patch_size': 16, 'in_chans': 3, 'embed_dim': 384, 'depth': 12, 'num_heads': 12, 'num_classes': 40, 'drop_rate': 0.0, 'pos_embed_interp': True, 'align_corners': False, 'mlp_ratio': 4.0, 'qkv_bias': 'True,', 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'random_init': False, 'distilled': False, 'moe_mlp_ratio': 1, 'moe_top_k': 4, 'gate_dim': 389, 'gate_return_decoupled_activation': False}, 'head': 'VisionTransformerUpHead', 'head_kwargs': {'in_channels': 1024, 'channels': 512, 'embed_dim': 384, 'img_size': [512, 512], 'align_corners': False, 'num_conv': 4, 'upsampling_method': 'bilinear', 'num_upsampe_layer': 4, 'num_classes': 40, 'in_index': 11, 'patch_size': 16, 'conv3x3_conv1x1': True}, 'task_dictionary': {'include_semseg': True, 'include_human_parts': True, 'include_sal': True, 'include_edge': True, 'include_normals': True, 'edge_w': 0.95}, 'loss_kwargs': {'loss_scheme': 'baseline', 'loss_weights': {'semseg': 1.0, 'human_parts': 2.0, 'sal': 5.0, 'edge': 50.0, 'normals': 10.0}}, 'eval_final_10_epochs_only': True, 'TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}, 'SINGLE_TASK_TEST_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_test_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_test_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_test_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_test_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_test_edge.json'}, 'SINGLE_TASK_VAL_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_val_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_val_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_val_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_val_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_val_edge.json'}}, 'normloss': 1, 'edge_w': 0.95, 'eval_edge': False, 'ALL_TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}}, 'TRAIN': {'SCALE': (512, 512)}, 'TEST': {'SCALE': (512, 512)}, 'overfit': False, 'multi_level': False, 'root_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None', 'output_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline', 'save_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/results', 'checkpoint': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/checkpoint.pth.tar', 'best_model': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/best_model.pth.tar', 'multi_gate': True, 'local_rank': 0, 'gpus': 2}
    in_channels: 1024
    channels: 512
    in_index: 11
    num_classes: 7
    align_corners: False
================================================================================
self.h 32 self.w 32
will consider multi level output in head False
will consider tam in heads True
head[human_parts] VisionTransformerUpHead(
  input_transform=None, ignore_index=255, align_corners=False
  (dropout): Dropout2d(p=0.1, inplace=False)
  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (conv_0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_4): Conv2d(256, 7, kernel_size=(1, 1), stride=(1, 1))
  (syncbn_fc_0): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
================================================================================
Creating: models.vit_up_head.VisionTransformerUpHead
  Arguments:
    img_size: [512, 512]
    patch_size: 16
    embed_dim: 384
    norm_cfg: {'type': 'SyncBN', 'requires_grad': True}
    num_conv: 4
    upsampling_method: bilinear
    num_upsampe_layer: 4
    conv3x3_conv1x1: True
    p: {'setup': 'multi_task', 'train_db_name': 'PASCALContext', 'val_db_name': 'PASCALContext', 'trBatch': 8, 'valBatch': 8, 'nworkers': 4, 'epochs': 5, 'optimizer': 'sgd', 'optimizer_kwargs': {'lr': 0.002, 'momentum': 0.9, 'weight_decay': 0.0001}, 'scheduler': 'poly', 'model': 'baseline', 'model_kwargs': {'tam': True, 'tam_level0': False, 'tam_level1': False, 'tam_level2': False}, 'backbone': 'VisionTransformer_moe', 'backbone_kwargs': {'dilated': False, 'model_name': 'vit_small_patch16_224', 'img_size': [512, 512], 'patch_size': 16, 'in_chans': 3, 'embed_dim': 384, 'depth': 12, 'num_heads': 12, 'num_classes': 40, 'drop_rate': 0.0, 'pos_embed_interp': True, 'align_corners': False, 'mlp_ratio': 4.0, 'qkv_bias': 'True,', 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'random_init': False, 'distilled': False, 'moe_mlp_ratio': 1, 'moe_top_k': 4, 'gate_dim': 389, 'gate_return_decoupled_activation': False}, 'head': 'VisionTransformerUpHead', 'head_kwargs': {'in_channels': 1024, 'channels': 512, 'embed_dim': 384, 'img_size': [512, 512], 'align_corners': False, 'num_conv': 4, 'upsampling_method': 'bilinear', 'num_upsampe_layer': 4, 'num_classes': 40, 'in_index': 11, 'patch_size': 16, 'conv3x3_conv1x1': True}, 'task_dictionary': {'include_semseg': True, 'include_human_parts': True, 'include_sal': True, 'include_edge': True, 'include_normals': True, 'edge_w': 0.95}, 'loss_kwargs': {'loss_scheme': 'baseline', 'loss_weights': {'semseg': 1.0, 'human_parts': 2.0, 'sal': 5.0, 'edge': 50.0, 'normals': 10.0}}, 'eval_final_10_epochs_only': True, 'TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}, 'SINGLE_TASK_TEST_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_test_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_test_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_test_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_test_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_test_edge.json'}, 'SINGLE_TASK_VAL_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_val_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_val_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_val_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_val_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_val_edge.json'}}, 'normloss': 1, 'edge_w': 0.95, 'eval_edge': False, 'ALL_TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}}, 'TRAIN': {'SCALE': (512, 512)}, 'TEST': {'SCALE': (512, 512)}, 'overfit': False, 'multi_level': False, 'root_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None', 'output_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline', 'save_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/results', 'checkpoint': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/checkpoint.pth.tar', 'best_model': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/best_model.pth.tar', 'multi_gate': True, 'local_rank': 0, 'gpus': 2}
    in_channels: 1024
    channels: 512
    in_index: 11
    num_classes: 1
    align_corners: False
================================================================================
self.h 32 self.w 32
will consider multi level output in head False
will consider tam in heads True
head[sal] VisionTransformerUpHead(
  input_transform=None, ignore_index=255, align_corners=False
  (dropout): Dropout2d(p=0.1, inplace=False)
  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (conv_0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_4): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
  (syncbn_fc_0): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
================================================================================
Creating: models.vit_up_head.VisionTransformerUpHead
  Arguments:
    img_size: [512, 512]
    patch_size: 16
    embed_dim: 384
    norm_cfg: {'type': 'SyncBN', 'requires_grad': True}
    num_conv: 4
    upsampling_method: bilinear
    num_upsampe_layer: 4
    conv3x3_conv1x1: True
    p: {'setup': 'multi_task', 'train_db_name': 'PASCALContext', 'val_db_name': 'PASCALContext', 'trBatch': 8, 'valBatch': 8, 'nworkers': 4, 'epochs': 5, 'optimizer': 'sgd', 'optimizer_kwargs': {'lr': 0.002, 'momentum': 0.9, 'weight_decay': 0.0001}, 'scheduler': 'poly', 'model': 'baseline', 'model_kwargs': {'tam': True, 'tam_level0': False, 'tam_level1': False, 'tam_level2': False}, 'backbone': 'VisionTransformer_moe', 'backbone_kwargs': {'dilated': False, 'model_name': 'vit_small_patch16_224', 'img_size': [512, 512], 'patch_size': 16, 'in_chans': 3, 'embed_dim': 384, 'depth': 12, 'num_heads': 12, 'num_classes': 40, 'drop_rate': 0.0, 'pos_embed_interp': True, 'align_corners': False, 'mlp_ratio': 4.0, 'qkv_bias': 'True,', 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'random_init': False, 'distilled': False, 'moe_mlp_ratio': 1, 'moe_top_k': 4, 'gate_dim': 389, 'gate_return_decoupled_activation': False}, 'head': 'VisionTransformerUpHead', 'head_kwargs': {'in_channels': 1024, 'channels': 512, 'embed_dim': 384, 'img_size': [512, 512], 'align_corners': False, 'num_conv': 4, 'upsampling_method': 'bilinear', 'num_upsampe_layer': 4, 'num_classes': 40, 'in_index': 11, 'patch_size': 16, 'conv3x3_conv1x1': True}, 'task_dictionary': {'include_semseg': True, 'include_human_parts': True, 'include_sal': True, 'include_edge': True, 'include_normals': True, 'edge_w': 0.95}, 'loss_kwargs': {'loss_scheme': 'baseline', 'loss_weights': {'semseg': 1.0, 'human_parts': 2.0, 'sal': 5.0, 'edge': 50.0, 'normals': 10.0}}, 'eval_final_10_epochs_only': True, 'TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}, 'SINGLE_TASK_TEST_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_test_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_test_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_test_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_test_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_test_edge.json'}, 'SINGLE_TASK_VAL_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_val_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_val_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_val_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_val_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_val_edge.json'}}, 'normloss': 1, 'edge_w': 0.95, 'eval_edge': False, 'ALL_TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}}, 'TRAIN': {'SCALE': (512, 512)}, 'TEST': {'SCALE': (512, 512)}, 'overfit': False, 'multi_level': False, 'root_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None', 'output_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline', 'save_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/results', 'checkpoint': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/checkpoint.pth.tar', 'best_model': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/best_model.pth.tar', 'multi_gate': True, 'local_rank': 0, 'gpus': 2}
    in_channels: 1024
    channels: 512
    in_index: 11
    num_classes: 3
    align_corners: False
================================================================================
self.h 32 self.w 32
will consider multi level output in head False
will consider tam in heads True
head[normals] VisionTransformerUpHead(
  input_transform=None, ignore_index=255, align_corners=False
  (dropout): Dropout2d(p=0.1, inplace=False)
  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (conv_0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_4): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
  (syncbn_fc_0): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
================================================================================
Creating: models.vit_up_head.VisionTransformerUpHead
  Arguments:
    img_size: [512, 512]
    patch_size: 16
    embed_dim: 384
    norm_cfg: {'type': 'SyncBN', 'requires_grad': True}
    num_conv: 4
    upsampling_method: bilinear
    num_upsampe_layer: 4
    conv3x3_conv1x1: True
    p: {'setup': 'multi_task', 'train_db_name': 'PASCALContext', 'val_db_name': 'PASCALContext', 'trBatch': 8, 'valBatch': 8, 'nworkers': 4, 'epochs': 5, 'optimizer': 'sgd', 'optimizer_kwargs': {'lr': 0.002, 'momentum': 0.9, 'weight_decay': 0.0001}, 'scheduler': 'poly', 'model': 'baseline', 'model_kwargs': {'tam': True, 'tam_level0': False, 'tam_level1': False, 'tam_level2': False}, 'backbone': 'VisionTransformer_moe', 'backbone_kwargs': {'dilated': False, 'model_name': 'vit_small_patch16_224', 'img_size': [512, 512], 'patch_size': 16, 'in_chans': 3, 'embed_dim': 384, 'depth': 12, 'num_heads': 12, 'num_classes': 40, 'drop_rate': 0.0, 'pos_embed_interp': True, 'align_corners': False, 'mlp_ratio': 4.0, 'qkv_bias': 'True,', 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'random_init': False, 'distilled': False, 'moe_mlp_ratio': 1, 'moe_top_k': 4, 'gate_dim': 389, 'gate_return_decoupled_activation': False}, 'head': 'VisionTransformerUpHead', 'head_kwargs': {'in_channels': 1024, 'channels': 512, 'embed_dim': 384, 'img_size': [512, 512], 'align_corners': False, 'num_conv': 4, 'upsampling_method': 'bilinear', 'num_upsampe_layer': 4, 'num_classes': 40, 'in_index': 11, 'patch_size': 16, 'conv3x3_conv1x1': True}, 'task_dictionary': {'include_semseg': True, 'include_human_parts': True, 'include_sal': True, 'include_edge': True, 'include_normals': True, 'edge_w': 0.95}, 'loss_kwargs': {'loss_scheme': 'baseline', 'loss_weights': {'semseg': 1.0, 'human_parts': 2.0, 'sal': 5.0, 'edge': 50.0, 'normals': 10.0}}, 'eval_final_10_epochs_only': True, 'TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}, 'SINGLE_TASK_TEST_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_test_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_test_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_test_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_test_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_test_edge.json'}, 'SINGLE_TASK_VAL_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_val_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_val_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_val_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_val_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_val_edge.json'}}, 'normloss': 1, 'edge_w': 0.95, 'eval_edge': False, 'ALL_TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}}, 'TRAIN': {'SCALE': (512, 512)}, 'TEST': {'SCALE': (512, 512)}, 'overfit': False, 'multi_level': False, 'root_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None', 'output_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline', 'save_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/results', 'checkpoint': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/checkpoint.pth.tar', 'best_model': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/best_model.pth.tar', 'multi_gate': True, 'local_rank': 0, 'gpus': 2}
    in_channels: 1024
    channels: 512
    in_index: 11
    num_classes: 1
    align_corners: False
================================================================================
self.h 32 self.w 32
will consider multi level output in head False
will consider tam in heads True
head[edge] VisionTransformerUpHead(
  input_transform=None, ignore_index=255, align_corners=False
  (dropout): Dropout2d(p=0.1, inplace=False)
  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (conv_0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_4): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
  (syncbn_fc_0): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (syncbn_fc_3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
================================================================================
Creating: models.models.MultiTaskModel
  Arguments:
    backbone: VisionTransformerMoE module
    decoders: ModuleDict module
    tasks: ['semseg', 'human_parts', 'sal', 'normals', 'edge']
    p: {'setup': 'multi_task', 'train_db_name': 'PASCALContext', 'val_db_name': 'PASCALContext', 'trBatch': 8, 'valBatch': 8, 'nworkers': 4, 'epochs': 5, 'optimizer': 'sgd', 'optimizer_kwargs': {'lr': 0.002, 'momentum': 0.9, 'weight_decay': 0.0001}, 'scheduler': 'poly', 'model': 'baseline', 'model_kwargs': {'tam': True, 'tam_level0': False, 'tam_level1': False, 'tam_level2': False}, 'backbone': 'VisionTransformer_moe', 'backbone_kwargs': {'dilated': False, 'model_name': 'vit_small_patch16_224', 'img_size': [512, 512], 'patch_size': 16, 'in_chans': 3, 'embed_dim': 384, 'depth': 12, 'num_heads': 12, 'num_classes': 40, 'drop_rate': 0.0, 'pos_embed_interp': True, 'align_corners': False, 'mlp_ratio': 4.0, 'qkv_bias': 'True,', 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'random_init': False, 'distilled': False, 'moe_mlp_ratio': 1, 'moe_top_k': 4, 'gate_dim': 389, 'gate_return_decoupled_activation': False}, 'head': 'VisionTransformerUpHead', 'head_kwargs': {'in_channels': 1024, 'channels': 512, 'embed_dim': 384, 'img_size': [512, 512], 'align_corners': False, 'num_conv': 4, 'upsampling_method': 'bilinear', 'num_upsampe_layer': 4, 'num_classes': 40, 'in_index': 11, 'patch_size': 16, 'conv3x3_conv1x1': True}, 'task_dictionary': {'include_semseg': True, 'include_human_parts': True, 'include_sal': True, 'include_edge': True, 'include_normals': True, 'edge_w': 0.95}, 'loss_kwargs': {'loss_scheme': 'baseline', 'loss_weights': {'semseg': 1.0, 'human_parts': 2.0, 'sal': 5.0, 'edge': 50.0, 'normals': 10.0}}, 'eval_final_10_epochs_only': True, 'TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}, 'SINGLE_TASK_TEST_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_test_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_test_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_test_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_test_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_test_edge.json'}, 'SINGLE_TASK_VAL_DICT': {'semseg': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/semseg/results/PASCALContext_val_semseg.json', 'human_parts': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/human_parts/results/PASCALContext_val_human_parts.json', 'sal': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/sal/results/PASCALContext_val_sal.json', 'normals': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/normals/results/PASCALContext_val_normals.json', 'edge': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/single_task/edge/results/PASCALContext_val_edge.json'}}, 'normloss': 1, 'edge_w': 0.95, 'eval_edge': False, 'ALL_TASKS': {'NAMES': ['semseg', 'human_parts', 'sal', 'normals', 'edge'], 'NUM_OUTPUT': {'semseg': 21, 'human_parts': 7, 'sal': 1, 'normals': 3, 'edge': 1}, 'FLAGVALS': {'image': 2, 'semseg': 0, 'human_parts': 0, 'sal': 0, 'normals': 2, 'edge': 0}, 'INFER_FLAGVALS': {'semseg': 0, 'human_parts': 0, 'sal': 1, 'normals': 1, 'edge': 1}}, 'TRAIN': {'SCALE': (512, 512)}, 'TEST': {'SCALE': (512, 512)}, 'overfit': False, 'multi_level': False, 'root_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None', 'output_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline', 'save_dir': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/results', 'checkpoint': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/checkpoint.pth.tar', 'best_model': '/home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/best_model.pth.tar', 'multi_gate': True, 'local_rank': 0, 'gpus': 2}
================================================================================
will consider tam in model True
will consider multi level output in model False
will consider multi gate output in model True
Use fast moe distributed learning==================>>
Get loss
Using L1 loss for surface normals
MultiTaskLoss(
  (loss_ft): ModuleDict(
    (semseg): SoftMaxwithLoss(
      (softmax): LogSoftmax(dim=1)
      (criterion): NLLLoss()
    )
    (human_parts): SoftMaxwithLoss(
      (softmax): LogSoftmax(dim=1)
      (criterion): NLLLoss()
    )
    (sal): BalancedCrossEntropyLoss()
    (normals): NormalsLoss(
      (normalize): Normalize()
    )
    (edge): BalancedCrossEntropyLoss()
  )
)
Retrieve optimizer
Optimizer uses a single parameter group - (Default)
optimizer SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.002
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.002
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)
Retrieve dataset
Preparing train loader for db: PASCALContext
Files already downloaded

Initializing dataloader for PASCAL train set
Number of images with human parts: 1736
Number of dataset images: 4998
Preparing val loader for db: PASCALContext
Files already downloaded

Initializing dataloader for PASCAL val set
Number of images with human parts: 1853
Number of dataset images: 5105
Preparing val loader for db: PASCALContext
Files already downloaded

Initializing dataloader for PASCAL val set
Number of images with human parts: 1853
Number of dataset images: 5105
Train samples 4998 - Val samples 5105
Train transformations:
Compose(
    RandomHorizontalFlip
    ScaleNRotate:(rot=(-20, 20),scale=(0.75, 1.25))
    FixedResize:{'image': (512, 512), 'semseg': (512, 512), 'human_parts': (512, 512), 'sal': (512, 512), 'normals': (512, 512), 'edge': (512, 512)}
    AddIgnoreRegions
    ToTensor
    Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
)
Val transformations:
Compose(
    FixedResize:{'image': (512, 512), 'semseg': (512, 512), 'human_parts': (512, 512), 'sal': (512, 512), 'normals': (512, 512), 'edge': (512, 512)}
    AddIgnoreRegions
    ToTensor
    Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
)
No checkpoint file at /home/jy/m3vit/output_dir/posemb_from_pretrain_True_moeexperts16_top4_pretrained_False_vmoe_noisy_std0.0_lr_None/PASCALContext/VisionTransformer_moe/vit_small_patch16_224/multi_task_baseline/checkpoint.pth.tar
Starting main loop
Epoch 1/5
----------
Adjusted learning rate to 0.00200
Train ...
